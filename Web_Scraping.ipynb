{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web Scraping",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNanVoVVSNtPDak7aj+oYFW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/knolasco/Web-Scraping-from-Craigslist/blob/main/Web_Scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeSQGzDdzxfx"
      },
      "source": [
        "def prepare_csv():\r\n",
        "  '''\r\n",
        "  This function prepares the csv file to be written\r\n",
        "  '''\r\n",
        "  fileName = 'scraping.csv'\r\n",
        "  f = open(fileName, 'w')\r\n",
        "  headers = 'product, price, location, date\\n'\r\n",
        "  f.write(headers)\r\n",
        "  return f\r\n",
        "\r\n",
        "def connect_and_open(my_url):\r\n",
        "  '''\r\n",
        "  This function establishes a connection and opens the page.\r\n",
        "  It will return the html page as a soup object\r\n",
        "  '''\r\n",
        "  uClient = uReq(my_url)\r\n",
        "  page_html = uClient.read()\r\n",
        "  uClient.close()\r\n",
        "  page_soup = soup(page_html)\r\n",
        "  return page_soup\r\n",
        "\r\n",
        "def scape_and_write(f, page_soup):\r\n",
        "  '''\r\n",
        "  containers holds the information for each item on the page\r\n",
        "  we will loop through different websites, open them, scrape them, and write into the csv\r\n",
        "  '''\r\n",
        "  containers = page_soup.findAll('li',{'class':'result-row'})\r\n",
        "  for container in containers:\r\n",
        "    item_name = container.div.h3.a.text\r\n",
        "    item_price = container.findAll('span', attrs = {'class':'result-price'})[0].text\r\n",
        "    try:\r\n",
        "      item_location = container.find('span',{'class':'result-hood'}).text\r\n",
        "    except:\r\n",
        "      item_location = 'NA'\r\n",
        "    try:\r\n",
        "      item_date = container.find('time',{'class':'result-date'}).text\r\n",
        "    except:\r\n",
        "      item_date = 'NA'\r\n",
        "    f.write(item_name.replace(',','') + ',' + item_price.replace(',','') + ',' + item_location.replace(',','') + ',' + item_date.replace(',','') + '\\n')\r\n",
        "  return f\r\n",
        "\r\n",
        "def loop_pages(my_url):\r\n",
        "  url_list = []\r\n",
        "  for i in range(13):\r\n",
        "    if i == 0:\r\n",
        "      url_list.append(my_url)\r\n",
        "    else:\r\n",
        "      my_url = 'https://losangeles.craigslist.org/d/for-sale/search/sss'\r\n",
        "      increment = '?s='\r\n",
        "      increment = increment + str(120*i)\r\n",
        "      my_url = my_url + increment\r\n",
        "      url_list.append(my_url)\r\n",
        "  return url_list\r\n",
        "\r\n",
        "def main():\r\n",
        "  from bs4 import BeautifulSoup as soup\r\n",
        "  from urllib.request import urlopen as uReq\r\n",
        "  f = prepare_csv()\r\n",
        "  my_url = 'https://losangeles.craigslist.org/d/for-sale/search/sss'\r\n",
        "  url_list = loop_pages(my_url)\r\n",
        "  for url in url_list:\r\n",
        "    page_soup = connect_and_open(url)\r\n",
        "    f = scape_and_write(f, page_soup)\r\n",
        "  f.close()\r\n",
        "  from google.colab import files\r\n",
        "  files.download('scraping.csv')\r\n",
        "  return\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "x-Ta6SR1AhbW",
        "outputId": "f326ee3d-4509-4321-85bd-c2933c6021fe"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "  main()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_78bf8ac1-2c1c-4685-9d79-0a9ee8df0523\", \"scraping.csv\", 101265)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}